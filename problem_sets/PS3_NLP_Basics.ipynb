{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z60REhCGFB01",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a4c41f39205051c2",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "# Problem Set 3 - Implementing Traditional NLP Techniques\n",
        "\n",
        "### DSME 6635: Artificial Intelligence for Business Research (Spring 2024)\n",
        "\n",
        "### Due at 12:30PM, Tuesday, February 6, 2024\n",
        "\n",
        "Please first copy the CoLab file onto your own Google Drive. Finish the questions below and submit the **CoLab link** of your solutions in [this Google Sheet](https://docs.google.com/spreadsheets/d/1nOE-saTptG73WMCONDB1Z3pt-jHhmDA_1OHpQVHqQ1M/edit#gid=306873990). The total achievable points are 8 for this problem set. Please name you solution as\n",
        "\n",
        "- `Member1LastName_Member1FirstName-Member2LastName_Member2FirstName_PS3.ipynb` (e.g., `Cao_Leo-Zhang_Renyu_PS3.ipynb`)\n",
        "\n",
        "In this problem set, we will implement a set of traditional NLP techniques. We will start from pre-processing of the textual information. We will then build a dictionary method to do sentiment classification. Last, we will build a Naive Bayes Classifier for sentiment classification.\n",
        "\n",
        "\n",
        "## 1. Pre-processing of Text\n",
        "\n",
        "In this question, you will implement various functions that help us to preprocess text information. Remember that before conducting any NLP tasks, we need to first convert sentence into words, which is a process called **tokenization**. In this exercise, you will implement a tokenization function to preprocess tweets. The function will take a tweet and conduct the following:\n",
        "\n",
        "1. The function will use [**`word_tokenize` from NLTK**](https://www.nltk.org/api/nltk.tokenize.html) to tokenize the documents into words. (We will use `TweetTokenizer` since it can help us preseve the hashtag #).\n",
        "2. The function will then get rid of all non-words (punctuations, emoji etc), which are not useful for sentiment classification.\n",
        "3. The function stem all the remaining words using the [NLTK PorterStemmer](https://www.nltk.org/howto/stem.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9n7tBwsFB01",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-d63b6e190094a504",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71HNEAbLFB02",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b78086c290d4a60c",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import twitter_samples\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import *\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "    \"\"\"\n",
        "    The function preprocess a tweet.\n",
        "    Input:\n",
        "        tweet: a string representing a raw tweet\n",
        "    Output:\n",
        "        tokens: a list of words from the tweet after pre-processing\n",
        "    \"\"\"\n",
        "    tokens = None\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pq772XTFB02",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-2853176972493b6e",
          "locked": true,
          "points": 15,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "assert preprocess_tweet('this is a test!') == ['thi', 'is', 'a', 'test']\n",
        "assert preprocess_tweet('work worked working') == ['work', 'work', 'work']\n",
        "assert preprocess_tweet('programming is so fun :)') == ['program', 'is', 'so', 'fun']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bi13yHWcFB03",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1c14a1786a26dc5a",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "## 2. Dictionary-Based Sentimemt Analysis\n",
        "\n",
        "In this question, you will implement the dictionary-based sentiment classifier. The first function you build will complete the following tasks:\n",
        "\n",
        "1. You will go through the training tweets and preprocess each one of it (sentence -> a list of words).\n",
        "\n",
        "2. You will then go through the words and labels and build a dictionary where the key is each unqiue word and the value is a list of two numbers representing how many times the words has appeared in positive or negative. Example: if 'good' appears 2 times in positive tweets and 0 times in negative tweets, the corresponding dictionary item is `{'good': [2, 0]}`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-1iEP0nFB03",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-562beecb93f03f23",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "def build_dictionary(pos_tweet, neg_tweet):\n",
        "    \"\"\"\n",
        "    This function builds a sentiment frequency dictionary from data\n",
        "    Input:\n",
        "        pos_tweet: a list of positive tweets\n",
        "        neg_tweet: a list of negative tweets\n",
        "    Output:\n",
        "        sentiment_dictionary: a dictionary where the key is the word and the value is\n",
        "        how many times a word appears in positive and negative tweets.\n",
        "    \"\"\"\n",
        "    pos_processed = []\n",
        "    neg_processed = []\n",
        "    sentiment_dictionary = {}\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return sentiment_dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qJQHu8yFB03",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-681343a4d7e22bbd",
          "locked": true,
          "points": 15,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "train_prob = 0.8\n",
        "num_train_data = int(len(all_positive_tweets)*train_prob)\n",
        "\n",
        "test_pos = all_positive_tweets[num_train_data:]\n",
        "train_pos = all_positive_tweets[:num_train_data]\n",
        "test_neg = all_negative_tweets[num_train_data:]\n",
        "train_neg = all_negative_tweets[:num_train_data]\n",
        "\n",
        "sentiment_dictionary = build_dictionary(train_pos, train_neg)\n",
        "\n",
        "assert sentiment_dictionary['top'] == [29, 4]\n",
        "assert sentiment_dictionary['hate'] == [9, 45]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W_d3ZmPFB03",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4d5ec4c0bab73eba",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "In the following function (`process_tweet`), you will take in a tweet and the sentiment dictionary and conduct the following two steps:\n",
        "\n",
        "1. You will preprocess each the tweet (sentence -> list of words).\n",
        "\n",
        "2. You will then generate a vector of **three** numbers representing the tweet: `[1, the sum of words' positive values, the sum of words' negative values]`, where the first row is always 1.\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85drQJ2DFB03",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-793fab3d9ca72124",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def process_tweets(dictionary,tweet):\n",
        "    \"\"\"\n",
        "    The function to process a tweet into three values\n",
        "    Input:\n",
        "        dictionary: a dictionary where the key is the word and the value is\n",
        "        how many times a word appears in positive and negative tweets.\n",
        "        tweet: the tweet in a string\n",
        "    Output:\n",
        "        returned_values: a list of three values, where the first is 1, the\n",
        "        second is the sum of positive values of words, the third is the sum\n",
        "        of negative values of words.\n",
        "    \"\"\"\n",
        "    returned_values = [1, 0, 0]\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return returned_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpvhcxXZFB04",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-402bf89f76c4bd42",
          "locked": true,
          "points": 15,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "assert process_tweets(sentiment_dictionary, 'him me') == [1, 306, 632]\n",
        "assert process_tweets(sentiment_dictionary, train_pos[1]) == [1, 4486, 3361]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-GMqDvjFB04",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ad59480542d658e0",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "The following function `lr_sentiemnt()` will take the dictionary and the positive as well as negative data to build a logistic classification model based on the positive and negative frequencies. You will then report your accuracy on the testing data. The logistics regression can be implemented through sklearn's logistics regression.\n",
        "\n",
        "In particular, you need to finish the following steps:\n",
        "\n",
        "1. Use `process_tweets()` to process each of the tweet in the positive and negative training samples.\n",
        "\n",
        "2. Build a logistics regression classifier where the features are the processed tweets (each tweet has 3 numbers), and the label is 1 if the tweet is positive otherwise 0. You will use the [sklearn package to build this logistic regressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) (<font color='red'>Please use random_state = 0 and solver='liblinear' for the logistics regression</font> to pass the unit test.)\n",
        "\n",
        "3. Evaluate your classifier on the testing data. Report the percentage of tweets that you successfully classified in both positive and negative testing tweets (i.e., accuracy for both positive and negative data). You should also report the overall accuracy of the training data (both positive and negative combined).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwvHStcnFB04",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ac1a01e466fc2ab8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "def lr_sentiment(dictionary, train_pos, train_neg, test_pos, test_neg):\n",
        "    \"\"\"\n",
        "    This function builds a Logistics classifer to classify tweets based on\n",
        "    frequency tables\n",
        "    Input:\n",
        "        train_pos, train_neg: positive and negative training tweets.\n",
        "        test_pos, test_neg: positive and negative testing tweets.\n",
        "        dictionary: a dictionary where the key is the word and the value is\n",
        "        how many times a word appears in positive and negative tweets.\n",
        "    Output:\n",
        "        nb_model: the variable storing the logistics classifier.\n",
        "        training_accuracy: the overall accuracy of the model over all training data.\n",
        "        pos_accuracy: the accuracy of classifying tweets in test_pos.\n",
        "        neg_accuracy: the accuracy of classifying tweets in test_neg.\n",
        "    \"\"\"\n",
        "\n",
        "    lr_model = None\n",
        "    pos_accuracy = neg_accuracy = training_accuracy = 0\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return lr_model, training_accuracy, pos_accuracy, neg_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q52mS8jLFB04",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-a026e131f46c9699",
          "locked": true,
          "points": 25,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "lr_model, training_accuracy, pos_accuracy, neg_accuracy = lr_sentiment(sentiment_dictionary, train_pos,\n",
        "                                                    train_neg, test_pos, test_neg)\n",
        "assert np.isclose(training_accuracy, 0.691375)\n",
        "assert np.isclose(pos_accuracy, 0.622)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUmVEXt-FB04",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-be05f8a3a7df7b80",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 3. Naive Bayes\n",
        "\n",
        "In this question, you will build a Naive Bayes Classifier to do sentiment classification on the same data. You are going to leverage the ([**NLTK's Naive Bayes Classifier**](https://www.nltk.org/_modules/nltk/classify/naivebayes.html)). In particular, the function you will build should finish the following steps:\n",
        "\n",
        "1. Pre-process the training and testing data. You need to use the pre-process function developed in Question 1 to change tweet to list of words.\n",
        "\n",
        "2. For each list of words, you need to translate it into a dictionary. The key of the dictionary is the unique words in tweet, the value is the number of times this word appearing in this tweet. And you should create a tuple where the first element is this dictionary and the second element is the label of the tweet. For positive tweet, the label should be 'pos'; and for negative tweet, the label should be 'neg'.\n",
        "\n",
        "3. You will then use the pre-processed data to train the Naive Bayes classifer from `nltk`.\n",
        "\n",
        "4. You will then use the Naive Bayes classifer you trained to classify each tweet in positive and negative testing tweets and report the positive and negative testing accuracies.\n",
        "\n",
        "**Note: This problem set may take a while to run since you are building a dictionary (the length of the dictionary = the number of unique words in the tweet) for each training and testing data point.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZclTtBjIFB04",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-da44fc66841a4e1e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def nb_sentiment(train_pos, train_neg, test_pos, test_neg):\n",
        "    \"\"\"\n",
        "    This function builds a Naive Bayes classifer to classify tweets.\n",
        "    Input:\n",
        "        train_pos, train_neg: positive and negative training tweets.\n",
        "        test_pos, test_neg: positive and negative testing tweets.\n",
        "    Output:\n",
        "        nb_model: the variable storing the naive bayes classifier.\n",
        "        pos_accuracy: the accuracy of classifying tweets in test_pos.\n",
        "        neg_accuracy: the accuracy of classifying tweets in test_neg.\n",
        "    \"\"\"\n",
        "    nb_model = None\n",
        "    pos_accuracy = neg_accuracy = 0\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return nb_model, pos_accuracy, neg_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKl_-SpNFB05",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-b8104387bec6a69e",
          "locked": true,
          "points": 30,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "nb_model, pos_accuracy, neg_accuracy = nb_sentiment(train_pos, train_neg, test_pos, test_neg)\n",
        "assert np.isclose(pos_accuracy, 0.782)\n",
        "assert np.isclose(neg_accuracy, 0.772)\n",
        "assert nb_model.labels() == ['pos', 'neg']\n",
        "assert nb_model.classify({'I':1, 'hate':1, 'the':1, 'world':1}) == 'neg'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End of Problem Set 3."
      ],
      "metadata": {
        "id": "QUjZ_ql1EPuq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WEuu1sOxETfm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "30px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "threshold": 4,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}