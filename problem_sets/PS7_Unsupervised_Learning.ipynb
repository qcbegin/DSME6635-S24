{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qcbegin/DSME6635-S24/blob/main/problem_sets/PS7_Unsupervised_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az7Mr2sRHphR",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a4c41f39205051c2",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "## Problem Set 7 - Unsupervised Learning (EM-Algorithm & Topic Modeling)\n",
        "\n",
        "### Due at 12:30PM, Tuesday, April 23, 2024\n",
        "\n",
        "Please first copy the CoLab file onto your own Google Drive. Finish the questions below and submit the **CoLab link** of your solutions in [this Google Sheet](https://docs.google.com/spreadsheets/d/1nOE-saTptG73WMCONDB1Z3pt-jHhmDA_1OHpQVHqQ1M/edit#gid=43704098). The total achievable points are 8 for this problem set. Please name you solution as\n",
        "\n",
        "- `Member1LastName_Member1FirstName-Member2LastName_Member2FirstName_PS7.ipynb` (e.g., `Cao_Leo-Zhang_Renyu_PS7.ipynb`)\n",
        "\n",
        "\n",
        "\n",
        "## 1. EM-Algorithm\n",
        "\n",
        "The first exercise will be unsupervised learning, specifically EM algorithm. In particular, we will solve a very simple problem on mixture of gaussian.\n",
        "\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        a list of numbers x_1, x_2, ..., x_n, suppose these numbers are drawn from two Gaussian distributions with variance 1 and mean mu_1 and mu_2\n",
        "   \n",
        "    Output:\n",
        "        the probability of each number belongs to each Gaussian distribution.\n",
        "    \"\"\"\n",
        "\n",
        "You will solve this problem using the famous [EM-Algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm).\n",
        "\n",
        "The first function you will implement is called the **E-step**, which calculates each data point's probability belonging to each Gaussian distribution. Suppose you have gaussian distribution with mean $\\mu_c$ and standard deviation $\\sigma_c$, and $\\pi_c$ is the probability that a point comes from distribution $c$. When you have points $x_i$, the E-step's equation is as follows:\n",
        "\n",
        "$$ r_{ic} = \\frac{\\pi_c N(x_i | \\mu_c, \\sigma_c)}{\\sum_{k=1}^K \\pi_k N(x_i | \\mu_k, \\sigma_k)} $$\n",
        "\n",
        "where\n",
        "\n",
        "$$ N(x_i, \\mu_c, \\sigma_c) = \\frac{1}{(2\\pi)^{n/2}\\sigma_c} exp\\left(\\frac{(x_i-\\mu_c)^2\\sigma_c^2}{2}\\right).$$\n",
        "\n",
        "In our case, the number of gaussian distribution is 2, so K = 2. To make your life easier, you can use scipy.stats.norm to compute the probability of a Gaussian distribution.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NcQsuHq7HphU",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b78086c290d4a60c",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "def e_step(data_points, mu_1, sigma_1, mu_2, sigma_2, pi=np.array([1/2, 1/2])):\n",
        "    \"\"\"\n",
        "    This function conducts the e-step for gaussian mixture model with 2 distributions.\n",
        "    Input:\n",
        "        data_points: the data points from two gaussian.\n",
        "        mu_1, sigma_, mu_2, sigma_2: the prior parameters for the two gaussian.\n",
        "        pi: the total weights for each gaussian\n",
        "    Output:\n",
        "        probs: a matrix representing each point's probabiltiies towards each gaussian.\n",
        "        probs[:, 0] is the probability of each point towards the first gaussian.\n",
        "    \"\"\"\n",
        "    probs = np.zeros((len(data_points), 2))\n",
        "    pi = np.array([1/2,1/2])\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    probs[:, 0] = pi[0] * norm(mu_1, sigma_1).pdf(data_points)\n",
        "    probs[:, 1] = pi[1] * norm(mu_2, sigma_2).pdf(data_points)\n",
        "    prob_sum = np.sum(probs, axis=1)\n",
        "    probs = probs / prob_sum[:, None]\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return probs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EXPynpfEHphV",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-edb04d32c9a4905f",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data_points = [0.1, 0.3, 10, 3]\n",
        "probs = e_step(data_points, 0, 1, 0, 1)\n",
        "assert np.array_equal(probs, np.ones((4, 2)) * 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4bSum8AHphW",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1c14a1786a26dc5a",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "The second function you will implement is the **M-step**. In particular, you will use the probabilities that each data point belongs to each distribution to update these distributions mean and standard deviations as the weighted averages of these data points (weighted by the probabilites).\n",
        "\n",
        "Suppose that $m_c$ is the total weights towards a distribution $c$, then you should compute based on following equations:\n",
        "$$ m_c = \\sum r_{ic}$$\n",
        "$$ \\pi_c = \\frac{m_c}{N}$$\n",
        "$$ \\mu_c = \\frac{1}{m_c}\\sum_i r_{ic} x_i $$\n",
        "$$ \\sigma_c^2 = \\frac{1}{m_c}\\sum_i r_{ic} (x_i - \\mu_c)^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8ffrhYsKHphW",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-562beecb93f03f23",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "def m_step(data_points, probs):\n",
        "    \"\"\"\n",
        "    This function performs the M-step for Gaussian Mixture Model.\n",
        "    Input:\n",
        "        data_points: a list of data points\n",
        "        probs: a matrix representing each data point's probabilties to the 2 distributions.\n",
        "    Output:\n",
        "        pi: the probability of each distribution in all data points.\n",
        "        mu_1, sigma_1, mu_2, sigma_2: estimated parameters for each distribution\n",
        "    \"\"\"\n",
        "    pi = mu_1 = mu_2 = sigma_1 = sigma_2 = 0\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    mu_1 = np.dot(probs[:, 0], data_points) / np.sum(probs[:, 0])\n",
        "    mu_2 = np.dot(probs[:, 1], data_points) / np.sum(probs[:, 1])\n",
        "    \n",
        "    sigma_1 = np.sqrt(np.dot(probs[:, 0], (data_points - mu_1) ** 2) / np.sum(probs[:, 0]))\n",
        "    sigma_2 = np.sqrt(np.dot(probs[:, 1], (data_points - mu_2) ** 2) / np.sum(probs[:, 1]))\n",
        "\n",
        "    pi = np.sum(probs, axis=0) / len(data_points)\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return pi, mu_1, sigma_1, mu_2, sigma_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FtoTHy5RHphX",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-681343a4d7e22bbd",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "data_points = [0.1, 0.3, 10, 3]\n",
        "probs = e_step(data_points, 0, 1, 10, 5)\n",
        "pi, mu_1, sigma_1, mu_2, sigma_2 = m_step(data_points, probs)\n",
        "assert np.isclose(pi[0], 0.51762712)\n",
        "assert np.isclose(mu_1, 0.3741766759682093)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JN3XztcHphX",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ad59480542d658e0",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Finally, combining the E-step and M-step, you will implement your EM-Algorithm that iteratively apply E and M steps until the probability distribution vector converges. You will return the final probability distribution as well as the mean and standard deviations of the two Gaussian random variables.\n",
        "\n",
        "In particular, your code should work as follows:\n",
        "\n",
        "1. Initialize two random Gaussians (may or may not be based on the data).\n",
        "\n",
        "2. Iterativelly apply EM-Algorithm until the L1-norm of probability distribution difference is smaller than the tolerance.\n",
        "\n",
        "3. Classify each point to one of the distirbution based on its probability distribution (which one is greater than 0.5).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ItiZ2SjUHphX",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ac1a01e466fc2ab8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def mixture_gaussian(data_points, tolerance = 0.00001):\n",
        "    \"\"\"\n",
        "    This function performs the overall gaussian mixture models.\n",
        "    Input:\n",
        "        data_points: a list of data poitns.\n",
        "        tolerance: the tolerance in stopping the EM algorithm.\n",
        "    Output:\n",
        "        pi: the probability of each distribution in all data points.\n",
        "        mu_1, sigma_1, mu_2, sigma_2: estimated parameters for each distribution\n",
        "    \"\"\"\n",
        "    pi = mu_1 = mu_2 = sigma_1 = sigma_2 = 0\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    # initialize the parameters\n",
        "    mu_1, mu_2 = np.random.choice(data_points, 2, replace=False)\n",
        "    sigma_1, sigma_2 = np.std(data_points), np.std(data_points)\n",
        "    probs_old = np.zeros((len(data_points), 2))\n",
        "\n",
        "    # iterate until convergence\n",
        "    while True:\n",
        "        # calculate the initial posterior probabilities (E-step)\n",
        "        probs = e_step(data_points, mu_1, sigma_1, mu_2, sigma_2)\n",
        "        # maximize the likelihood (M-step)\n",
        "        pi, mu_1, sigma_1, mu_2, sigma_2 = m_step(data_points, probs)\n",
        "        # check the L1 norm of the difference between the new and old probabilities\n",
        "        diff = np.linalg.norm(probs - probs_old, ord=1)\n",
        "        if diff < tolerance:\n",
        "            break\n",
        "\n",
        "        probs_old = probs.copy()\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return pi, mu_1, sigma_1, mu_2, sigma_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yGHyPAg2HphY",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-a026e131f46c9699",
          "locked": true,
          "points": 30,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "mu_1, sigma_1, mu_2, sigma_2 = 0, 1, 100, 10\n",
        "data_points= np.concatenate([np.random.normal(mu_1, sigma_1, 3000),\n",
        "                             np.random.normal(mu_2, sigma_2, 3000)])\n",
        "pi, mu_1_e, sigma_1_e, mu_2_e, sigma_2_e = mixture_gaussian(data_points)\n",
        "assert abs(mu_1_e-mu_1) < 0.1\n",
        "assert abs(mu_2_e-mu_2) < 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbh6aUMDHphY",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-be05f8a3a7df7b80",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 2. Topic Modeling\n",
        "\n",
        "Now you have seen a typical unsupervised learning algorithm (i.e., Gaussian Mixture Models), let us now focus on one of the most useful unsupervised learning algorithms, topic modeling. In this exercise, you will be using another popular NLP package in Python, [**gensim**](https://github.com/RaRe-Technologies/gensim), to conduct topic modeling.\n",
        "\n",
        "You can follow the guide [**here**](https://www.machinelearningplus.com/nlp/gensim-tutorial/).\n",
        "\n",
        "In order to complete this task, you need to do the following in principle:\n",
        "\n",
        "1. Use gensim's API to download the dataset.\n",
        "2. Pre-process the data by removing stopwords and stemmizing them.\n",
        "3. Create a gensim dictionary to reprseent each word and change the data into a gensim corpus.\n",
        "4. Apply the LDA model on corpus and the dictionary.\n",
        "5. Return the topics.\n",
        "\n",
        "Luckily, you don't need to do Step 1 to 3, since you have already done this in prior exercises with other packages (Flair and Keras). You can focus on Steps 4 and 5.\n",
        "\n",
        "We will be using the [**NYT article data from Kaggle**](https://www.kaggle.com/datasets/tumanovalexander/nyt-articles-data?resource=download&select=df_2019.csv).\n",
        "\n",
        "**Note: You will need to upload `df_2019.csv` into your Google Colab notebook.**\n",
        "\n",
        "**Note: This exercise will take some time since you are performing LDA updates on 5000 sentences.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ObNrGkbHHphY",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-da44fc66841a4e1e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/Xinyu/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /Users/Xinyu/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/Xinyu/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "# We need wordnet for lemmatizer\n",
        "\n",
        "# Pre-processing function\n",
        "lemma = WordNetLemmatizer()\n",
        "def preprocess(doc):\n",
        "    stop_free = \" \".join([i for i in doc.lower().split() if i not in set(stopwords.words('english'))])\n",
        "    punc_free = ''.join(ch for ch in stop_free if ch not in set(string.punctuation))\n",
        "    normalized = [lemma.lemmatize(word) for word in punc_free.split()]\n",
        "    return normalized\n",
        "\n",
        "# Load the NYT dataset and preprocess the fist 5000 articles\n",
        "nyt_df = pd.read_csv(\"../Data/df_2019.csv\")\n",
        "documents = nyt_df['sentence'].tolist()\n",
        "doc_clean = [preprocess(doc) for doc in documents[0:5000]]\n",
        "\n",
        "\n",
        "def topic_modeling(doc_clean, num_topics=7):\n",
        "    \"\"\"\n",
        "    This function takes a list of documents and return a gensim LDA model.\n",
        "    Input:\n",
        "        doc_clean: a list of documents.\n",
        "        num_topics: number of topics in the docuemnt.\n",
        "    Oputput:\n",
        "        ldamodel: the LDA model from gensim based on doc_clean\n",
        "    \"\"\"\n",
        "    # build the word to number dictionary\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "\n",
        "    # change each document into the term matrix, we can use tfidf here if wanted\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "\n",
        "    ldamodel = None\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus=doc_term_matrix,\n",
        "                                               id2word=dictionary,\n",
        "                                               num_topics=num_topics,\n",
        "                                               random_state=100,\n",
        "                                               update_every=1,\n",
        "                                               chunksize=100,\n",
        "                                               passes=10,\n",
        "                                               alpha='auto',\n",
        "                                               per_word_topics=True)\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return ldamodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UFzkOc5eHphZ",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-b8104387bec6a69e",
          "locked": true,
          "points": 30,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0,\n",
              "  '0.050*\"trump\" + 0.047*\"president\" + 0.017*\"leader\" + 0.016*\"wall\" + 0.015*\"right\" + 0.015*\"border\" + 0.012*\"house\" + 0.012*\"world\" + 0.011*\"school\" + 0.011*\"mr\"'),\n",
              " (1,\n",
              "  '0.065*\"new\" + 0.020*\"york\" + 0.015*\"first\" + 0.014*\"book\" + 0.013*\"make\" + 0.013*\"may\" + 0.012*\"home\" + 0.012*\"city\" + 0.011*\"look\" + 0.011*\"back\"'),\n",
              " (2,\n",
              "  '0.030*\"said\" + 0.026*\"say\" + 0.022*\"state\" + 0.017*\"woman\" + 0.016*\"american\" + 0.012*\"group\" + 0.011*\"united\" + 0.011*\"former\" + 0.009*\"many\" + 0.008*\"country\"'),\n",
              " (3,\n",
              "  '0.026*\"work\" + 0.017*\"offer\" + 0.017*\"art\" + 0.016*\"deal\" + 0.014*\"number\" + 0.012*\"attack\" + 0.012*\"line\" + 0.010*\"harris\" + 0.009*\"theater\" + 0.009*\"dead\"'),\n",
              " (4,\n",
              "  '0.049*\"shutdown\" + 0.033*\"government\" + 0.015*\"union\" + 0.015*\"history\" + 0.015*\"politics\" + 0.015*\"love\" + 0.014*\"case\" + 0.014*\"democratic\" + 0.013*\"address\" + 0.012*\"worker\"'),\n",
              " (5,\n",
              "  '0.025*\"show\" + 0.020*\"would\" + 0.017*\"take\" + 0.016*\"like\" + 0.014*\"company\" + 0.014*\"open\" + 0.013*\"start\" + 0.011*\"season\" + 0.010*\"oscar\" + 0.010*\"month\"'),\n",
              " (6,\n",
              "  '0.031*\"day\" + 0.027*\"u\" + 0.017*\"fall\" + 0.015*\"help\" + 0.014*\"china\" + 0.012*\"court\" + 0.011*\"briefing\" + 0.009*\"wednesday\" + 0.009*\"face\" + 0.008*\"federal\"'),\n",
              " (7,\n",
              "  '0.028*\"—\" + 0.028*\"one\" + 0.019*\"year\" + 0.019*\"it’s\" + 0.019*\"people\" + 0.018*\"get\" + 0.017*\"time\" + 0.016*\"review\" + 0.014*\"life\" + 0.012*\"spring\"'),\n",
              " (8,\n",
              "  '0.043*\"2019\" + 0.018*\"plan\" + 0.018*\"collection\" + 0.013*\"sexual\" + 0.012*\"play\" + 0.011*\"january\" + 0.011*\"child\" + 0.010*\"charge\" + 0.010*\"since\" + 0.010*\"made\"'),\n",
              " (9,\n",
              "  '0.022*\"need\" + 0.014*\"white\" + 0.014*\"want\" + 0.014*\"film\" + 0.014*\"know\" + 0.013*\"even\" + 0.012*\"here’s\" + 0.012*\"behind\" + 0.011*\"“the\" + 0.011*\"m\"')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ldamodel = topic_modeling(doc_clean, 10)\n",
        "assert len(ldamodel.print_topics()) == 10\n",
        "ldamodel.print_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NK97asVODVF5"
      },
      "source": [
        "# End of Problem Set 7."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D5ktebjDYmK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "30px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "threshold": 4,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
