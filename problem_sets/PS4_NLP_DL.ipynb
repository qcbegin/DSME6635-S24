{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qcbegin/DSME6635-S24/blob/main/problem_sets/PS4_NLP_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYylFn81GU4Y",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a4c41f39205051c2",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "# Problem Set 4 - Word2Vec and LSTM for Sentiment Analysis\n",
        "\n",
        "### Due at 12:30PM, Tuesday, March 5, 2024\n",
        "\n",
        "Please first copy the CoLab file onto your own Google Drive. Finish the questions below and submit the **CoLab link** of your solutions in [this Google Sheet](https://docs.google.com/spreadsheets/d/1nOE-saTptG73WMCONDB1Z3pt-jHhmDA_1OHpQVHqQ1M/edit#gid=840097885). The total achievable points are 8 for this problem set. Please name you solution as\n",
        "\n",
        "- `Member1LastName_Member1FirstName-Member2LastName_Member2FirstName_PS4.ipynb` (e.g., `Cao_Leo-Zhang_Renyu_PS4.ipynb`)\n",
        "\n",
        "In this problem set, we will start implementing a set of NLP techniques in the deep learning era. We will start from implementing the word2vec model for sentiment classification. Specifically, you will build your own deep learning model where the first layer is an embedding layer, the second layer is an LSTM layer of 128 words, and this LSTM layer fits into the last layer which is a sigmoid. This sigmoid layer is then used to predict the probabbility of text sentiment.\n",
        "\n",
        "We will use the continuous bag of word (CBOW) architecture and the tweet data, and will then visualize some of the word vectors.\n",
        "\n",
        "We will then leverage on existing NLP libraries to run sentiment classification using word2vec model.\n",
        "\n",
        "\n",
        "## 1. Word Vectors for Sentiment Classification\n",
        "To make your life slightly easier, we will use [**Keras**](https://keras.io/), instead of PyTorch, to build a word to vector model to classify the sentiment.\n",
        "\n",
        "The first function will read in the famous [IMDB review data from Keras](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb), and put it into `X_train`, `y_train`, `X_test`, `y_test`. In `X_train` and `X_test`, each point is a list of numbers where each number basically represents a unique word in the entire vocabulary.\n",
        "\n",
        "We will then use the `pad_sequence()` function to change each sequence in the training and testing data into equal length that is set by `maxlen` (by default 96). You can find the documentation of this function [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8X3Wh9B2GU4b",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b78086c290d4a60c",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "\n",
        "def load_data(num_words=20000, maxlen=96):\n",
        "    \"\"\"\n",
        "    This function load the imdb data.\n",
        "    Input:\n",
        "        num_words: the number of unique words we will consider with highest frequency.\n",
        "        maxlen: the maximum length of each sentence.\n",
        "    Output:\n",
        "        X_train, X_test: a list of items, each item has a list of numbers where each\n",
        "        number represents a word.\n",
        "        y_train, y_test: a list of one or zeros where one represents positive review\n",
        "        and zero represents negative review.\n",
        "    \"\"\"\n",
        "    X_train = y_train = X_test = y_test = None\n",
        "    ### BEGIN SOLUTION\n",
        "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "    X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "    X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "    ### END SOLUTION\n",
        "    return X_train, y_train, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y5Bt-erYGU4c",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-edb04d32c9a4905f",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "X_train, y_train, X_test, y_test = load_data()\n",
        "assert X_train.shape == (25000, 96)\n",
        "assert X_test.shape == (25000, 96)\n",
        "assert X_train[0][0] == 12\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdoIEFivGU4c",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1c14a1786a26dc5a",
          "locked": true,
          "schema_version": 3,
          "solution": false
        }
      },
      "source": [
        "## 2. Word Indices to Sentence\n",
        "\n",
        "Now we are going to implement a function that helps to see the actual sentence instead of word numbers from the dataset. In particular, you will need to use the [`get_word_index()`](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/get_word_index) function from the imdb dataset from keras to get the word corresponding to each number. We will then take a list of numbers, translate these numbers into actual words, and put these words into one string sequentially separated by space.  \n",
        "\n",
        "**Remember we need to get rid of the padding first!**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7FD-EaQvGU4d",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-562beecb93f03f23",
          "locked": false,
          "schema_version": 3,
          "solution": true
        }
      },
      "outputs": [],
      "source": [
        "def find_sentence(train_sentence):\n",
        "    \"\"\"\n",
        "    This function takes a list of numbers in IMDB data and translates it into words.\n",
        "    Input:\n",
        "        train_sentence: a list of numbers where numbers represent words\n",
        "    Output:\n",
        "        final_sentence: a string consists of words in the list in sequential\n",
        "        order and separated by space.\n",
        "    \"\"\"\n",
        "    final_sentence = \"\"\n",
        "    ### BEGIN SOLUTION\n",
        "    word_index = imdb.get_word_index()\n",
        "    index_word = {v: k for k, v in word_index.items()}\n",
        "    index_word[0] = '[PAD]'\n",
        "    final_sentence = ' '.join(index_word[i] for i in train_sentence if i != 0)\n",
        "    ### END SOLUTION\n",
        "    return final_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-_CpePOoGU4d",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-681343a4d7e22bbd",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false
        }
      },
      "outputs": [],
      "source": [
        "assert find_sentence(X_test[0])[0:23] == 'the wonder own as by is'\n",
        "assert find_sentence(X_test[10])[0:23] == 'and movie is him actual'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niORQ5FqGU4d",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ad59480542d658e0",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 3. Word2Vec Classification Model\n",
        "\n",
        "Now, in this function, we will build the deep learning model to classify the sentence sentiment based on the data we just get. In particular, we will finish the following steps:\n",
        "\n",
        "1. Build a model which has 3 major stages. The first stage is a simple embedding layer that translates the list of word numbers into an embedding with length set by `model_width` (by default 128). The second stage is an LSTM with 128 neuron units and 0.2 dropout rate. The last stage is a single dense layer that uses sigmoid as the activation function and outputs one number as the probability of the sentiment being positive.\n",
        "\n",
        "2. We will compile the model with `adam` optimizer and `binary_crossentropy` loss function. We will then use `accuracy` as our metrics, and 20% of the data as our validation split.\n",
        "\n",
        "3. We fit the model with training features and labels with a `batch_size` equal to `batch_size` and `epoch` equal to `epoch_number`.\n",
        "\n",
        "4. We evaluate the model based on testing features and labels.\n",
        "\n",
        "5. We return the model as well as the testing accuracy.\n",
        "\n",
        "**Note: This will take some time depending on how much computing resource Google allocates to you. You are training a relatively large LSTM network with a moderate embedding on 25,000 reviews. So each epoch will take one to two minutes at least.**\n",
        "\n",
        "The model should look similar to the following:\n",
        "\n",
        "| Layer (type)       |  Output Shape  | Paramerters |\n",
        "| ----------- | ----------- | --------- |\n",
        "|  embedding (Embedding)      | (None, None, 128)       | 2,560,000 |\n",
        "|  lstm (LSTM)   | (None, 128)        | 131,584 |\n",
        "| dense (Dense)      |         (None, 1)       |          129 |\n",
        " : Total params: 2,691,713\n",
        " : Trainable params: 2,691,713\n",
        " : Non-trainable params: 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OhTvCqUvGU4e",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-ac1a01e466fc2ab8",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def lstm_sentiment(X_train, y_train, X_test, y_test, model_width=128, num_words=20000,\n",
        "                   batch_size=32, epoch_number=5):\n",
        "    \"\"\"\n",
        "    This function builds an LSTM classify to find the sentiment.\n",
        "    Input:\n",
        "        X_train, y_train, X_test, y_test: training and testing data\n",
        "        model_width: the length of embeddings we use to represent words\n",
        "        num_words: the total number of unique words in our data.\n",
        "        batch_size: batch_size in processing data\n",
        "        epoch_number: how many times we go over the data.\n",
        "    Output:\n",
        "        model: the LSTM sentiment classifer we trained.\n",
        "        score: the validation score/cross-entropy loss of the model.\n",
        "        testing_accuracy: the accuracy of the model on testing data.\n",
        "    \"\"\"\n",
        "    training_accuracy = testing_accuracy = 0\n",
        "    model = None\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    # initialize the model\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(num_words, model_width))\n",
        "    model.add(LSTM(model_width, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # train the model\n",
        "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epoch_number, validation_split=0.2)\n",
        "\n",
        "    # evaluate the model\n",
        "    score = model.evaluate(X_test, y_test, batch_size=batch_size, return_dict=True)\n",
        "    testing_accuracy = score['accuracy']\n",
        "    \n",
        "    ### END SOLUTION\n",
        "\n",
        "    return model, score, testing_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "K5lt4ziaGU4e",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-a026e131f46c9699",
          "locked": true,
          "points": 30,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 53ms/step - accuracy: 0.7247 - loss: 0.5448 - val_accuracy: 0.8268 - val_loss: 0.4033\n",
            "Epoch 2/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.8805 - loss: 0.3022 - val_accuracy: 0.8236 - val_loss: 0.3938\n",
            "Epoch 3/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 51ms/step - accuracy: 0.9164 - loss: 0.2202 - val_accuracy: 0.8216 - val_loss: 0.4921\n",
            "Epoch 4/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 52ms/step - accuracy: 0.9399 - loss: 0.1606 - val_accuracy: 0.8236 - val_loss: 0.4693\n",
            "Epoch 5/5\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 52ms/step - accuracy: 0.9603 - loss: 0.1105 - val_accuracy: 0.8230 - val_loss: 0.5424\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.8173 - loss: 0.5578\n"
          ]
        }
      ],
      "source": [
        "model, score, testing_accuracy = lstm_sentiment(X_train, y_train, X_test, y_test)\n",
        "assert len(model.layers) == 3\n",
        "assert list(model.layers[0].weights[0].shape) == [20000, 128]\n",
        "assert len(model.layers[1].weights) == 3\n",
        "assert list(model.layers[1].weights[1].shape) == [128, 512]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q6aYLdkGU4f",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-be05f8a3a7df7b80",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 4. Flair\n",
        "\n",
        "[Flair](https://github.com/flairNLP/flair) is a reasonably update-to-date sentiment classifier using word embeddings. In those packages, you will find out word vectors as well as classifiers that have been trained by others. There are in general three ways to use these pacakges/pre-trained models:\n",
        "\n",
        "1. The first way is to directly use the model to conduct the task, which is what we will do here.\n",
        "\n",
        "2. The second way is to use the pre-trained word vectors or other raw materials in pre-processing texts as an input of your own model. Imagine that you can repeat the previous task but, instead of training an embedding layer, you can use the word vectors from Flair at the beginning.\n",
        "\n",
        "3. You can use pre-trained models and change the last part of the model to fit your own data. This is a process called transfer learning or fine-tuning, and has been utilized a lot in modern transformer-based models.\n",
        "\n",
        "\n",
        "In this exercise, we will use the IMDB dataset but the sentiment classifier trained by Flair. [Here](https://flairnlp.github.io/docs/intro) is the documentation of the classifier and you can find the [code here](https://github.com/flairNLP/flair/blob/6b6b2f0ebcc078328f4b349f58f3f3b77e99072d/flair/models/text_classification_model.py).\n",
        "\n",
        "    \"\"\"\n",
        "    Text Classification Model\n",
        "    The model takes word embeddings, puts them into an RNN to obtain a text\n",
        "    representation, and puts the text representation in the end into a linear\n",
        "    layer to get the actual class label. The model can handle single and multi\n",
        "    class data sets.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "In the first function below, before conducting the analysis, we have to reverse the imdb data from `X_test` to a list of words since the current data is not useable directly by Flair. In particular, we will take each item in `X_test` and use the function `find_sentence_2()` to change it into a string of words concatenated together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DhvMP3lcGU4f",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-da44fc66841a4e1e",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "def revert_data_to_text(original_data):\n",
        "    \"\"\"\n",
        "    This function takes a list of original IMDB data and returns a list\n",
        "    of strings.\n",
        "    Input：\n",
        "        original_data: a list original numeric data from IMDB dataset.\n",
        "    Output:\n",
        "        new_data: a list of strings where each string consists words represented\n",
        "        by the original list.\n",
        "    \"\"\"\n",
        "    new_data = []\n",
        "    word_index = imdb.get_word_index()\n",
        "    inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n",
        "\n",
        "    def find_sentence_2(train_sentence):\n",
        "        return \" \".join(inverted_word_index[i] for i in train_sentence if i !=0)\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    new_data = [find_sentence_2(i) for i in original_data]\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oUfuPK2jGU4f",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-b8104387bec6a69e",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "X_test_2 = revert_data_to_text(X_test)\n",
        "assert len(X_test_2) == 25000\n",
        "assert X_test_2[0][0:10] == 'the wonder'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZDuxyd4GU4f",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-1ed5585a9a7ef27d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "In the following function, we will use flair to do sentiment classification and report the out-of-sample accuracy. In particular, we will do:\n",
        "\n",
        "1. For each setence, we will first create a Flair sentence object to change it into Flair's internal data structure.\n",
        "\n",
        "2. We will then create a Flair sentiment classifier model in English using `en-sentiment`.\n",
        "\n",
        "3. For each Flair sentence, we will use the Flair model to classify it and save the results.\n",
        "\n",
        "4. We will then compare the results with the true labels to report the accuracy of Flair classifiers.\n",
        "\n",
        "Again, you can find more documentation about flair on [their GitHub page](https://github.com/flairNLP/flair) and [tutorial](https://flairnlp.github.io/docs/intro)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMQRwcHcI4R9"
      },
      "outputs": [],
      "source": [
        "!pip install flair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "pG_iQjgwGU4f",
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-35510308aec8424f",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import flair\n",
        "\n",
        "def flair_classification(X_test, y_test):\n",
        "    \"\"\"\n",
        "    This function uses Flair to do sentiment classification.\n",
        "    Input:\n",
        "        X_test, y_test: testing data\n",
        "    Output:\n",
        "        classification: a list of 1s and 0s where 1 represents the test data\n",
        "        is positive and 0 otherwise.\n",
        "        accuracy: the total accuracy of the model.\n",
        "    \"\"\"\n",
        "    classification = []\n",
        "    accuracy = 0\n",
        "\n",
        "    ### BEGIN SOLUTION\n",
        "    \n",
        "    # prepare the data and load the model\n",
        "    sentences = [flair.data.Sentence(i) for i in X_test]\n",
        "    tagger = flair.models.TextClassifier.load('en-sentiment')\n",
        "\n",
        "    # predict the sentiment\n",
        "    tagger.predict(sentences)\n",
        "\n",
        "    # calculate the accuracy\n",
        "    classification = [1 if s.labels[0].value == 'POSITIVE' else 0 for s in sentences]\n",
        "    accuracy = np.mean(classification == y_test)\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    return classification, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "qwLGWypsGU4g",
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-a161740abd434ab6",
          "locked": true,
          "points": 20,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "classification, accuracy = flair_classification(X_test_2[0:2500], y_test[0:2500])\n",
        "assert np.isclose(accuracy, 0.5232)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCQ6pK_SYMZ9"
      },
      "source": [
        "## End of Problem Set 4."
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "30px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "threshold": 4,
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
